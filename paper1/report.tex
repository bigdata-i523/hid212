\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Big Data Analysis Using Mapreduce}


\author{Saurabh Kumar}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Indiana University}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47408}
}
\email{kumarsau@iu.edu}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}
With the amount of data created everyday reaching exorbitant limits and a push for analysis across all major fields gaining traction, we need an advanced framework to analyze the large data. Hadoop is such a framework for developing distribution applications. MapReduce is an application model for Hadoop and other distributed file systems. Word count is a small analysis that can be done using MapReduce. It can also be used for complex tasks like implementing search functionality. The main implementation of MapReduce is processing and generating large data sets. The user created map function processes a key value pair. This produces intermediate key value pair. The reduce function does a group by on all the intermediate values based on their key. This programming model parallelizes all the code which then is executed on a cluster.
\end{abstract}

\keywords{HID212, i523, MapReduce, Distributed, Fault Tolerance, Master, Big Data}


\maketitle

\section{Introduction}

Many MapReduce models are developed for handling big data. Google distributed file system is one of them \cite{rf1}. Hadoop is the most popular open source framework for handling big data. Google's MapReduce framework has allowed the search for million of pages and delivering the results in milliseconds \cite{rf2}. The input data to be processed is large and the computation times have to be lowered. Therefore the data and the computations are distributed across thousands of machines to achieve the goal of low computation time while handling large data. The main problems that such a programming model should handle are data distribution, handling failures, parallelizing computation and fast execution. MapReduce handles all these complexities by providing an abstraction which in turn is done by hiding the data distribution details, error handling, parallelization and load balancing \cite{rf3}. 

The map and reduce functions help to parallelize the computations and re-execution is used for fault tolerance. The map function is applied to every record in input. This creates an intermediate key value pair to which a reduce function is applied. All the intermediate values with same key go to the same reducer. The number of mapper depends on the number of distributed file system blocks and the amount of data. Usually there are 10 to 100 mappers for each data node. The execution time on each machine should not be very high or low, as the task setup takes a while. The number reducer depends on the requirement of final data output and the number of unique keys in the intermediate data. The number of reducers should not be too high as it would slow down the computation by generating stragglers. We will see this later. The number should also not be too low so that parallelization cannot take its effect \cite{rf2}. 

The MapReduce paradigm works with a distributed file system like hadoop file system. Such a file system is different from a relational database system. The distributed file system has a NoSQL database which is suitable for processing big data \cite{rf5}.

Multiple groups of tasks can be processed by the MapReduce model. Although, optimization should be made as the communication cost between mappers and reducers can be very high for such a model \cite{rf6}.

MapReduce is a widely accepted paradigm for analysis on big data, but it also has some limitations. It cannot be easily used for complex tasks and algorithms. It is also not suitable for tasks that require multiple levels of processing \cite{rf7}.

\section{Model}
The computation is divided as two functions: Map and Reduce. The Map function, defined by the user, produces an intermediate key value pair. These values are grouped together according to their keys by the MapReduce library and then passed to the Reduce function. The group by produces a pair an intermediate key and all the values associated with that key. This pair is sent to the reduce function. These values are merged together by the reduce function and the values are supplied to the function in different iterations. So that when there are large list of values, the memory does not get full \cite{rf2}. 

To better understand this procedure we take the example of word frequency in a document. The Map function produces an intermediate key value pair of a word and its respective count of occurrences. The reduce function then groups by each word and sum up all the counts of occurrences. The final result is every word and its respective frequency in the document.  This procedure can also be shown by the help of the two equations below \cite{rf2}:

\[map    (k_1,v_1) \rightarrow  list(k_2,v_2) \]
\[reduce (k_2,list(v_2)) \rightarrow sum(v_2)\]

There are other applications of MapReduce. The map function produces web page requests and the respective output as the key value pair. The reduce function sums up all the counts for a specific page and returns a final output of the pair(URL, total count) \cite{rf2}. 

The map function produces a target and source pair for each link of a target URL in a source page. The reduce function produces the same key, target, and the value as a concatenated list of all the sources that belong to the same target. The output pair is (target, list(source)) \cite{rf4}.

 Another example can be inverted index. The map function parses every document to produce a key value pair.  A unique word becomes a key and the document id becomes the value.  The reduce function takes this as an input and outputs a sorted list of document for every word \cite{rf2}. 
\section{Implementation}
 The type of implementation of MapReduce depends on the environment. The implementation is different from a small shared memory machine to a large cluster of machines.  A cluster consists of hundreds of machines. When a job is submitted to a scheduler, it assigns each task in the job to an available machine in the cluster.
\subsection{Overview}  
Different machines in the cluster act as a mapper, that is they implement the map function. The data in partitioned into sets of M splits, where M is the number of mappers. Each split of data is fed to an individual mapper to parallel process each split. There are few machines that implement the reduce function. They are called reducers. The intermediate key space is split into R partitions, where R is the number of reducers. The number of partitions is provided by the user \cite{rf2}. 

The user invokes the MapReduce function. Then the MapReduce library splits the data into M partitions, where each partition size ranges from 16 megabytes to 64 megabytes. This size can be specified by the user through a parameter. Then the library starts some copies of the program on the cluster. One copy is the master code. The rest are workers or slaves which are assigned tasks by the master \cite{rf2}. 

The worker who is assigned a map function is a mapper. A mapper reads the contents of an input split. The mapper produces an intermediate key value pair. This pair is buffered in memory. The buffered data is orderly written to the local disk and divided into R splits. Location of this data is passed back to the master. The master in turns passes the location to the workers who are assigned reduce functions \cite{rf2}. 

The master notifies the reducer a location. Data is read from the local disk using a remote procedure call. The intermediate data read by the reducer is sorted and groups together data from each key. Each unique key is processed by the reducer. The reducer passes the key value pair to each reduce function. After all tasks are completed, the master wakes up the user program \cite{rf2}. 

After the completion of this process the final output is available in R files. Each file comes from one reduce task and the names are specified by the user.   These files can be combined to a single file or can be passed as input to another MapReduce function \cite{rf2}. 

The master stores some meta data. It has the state information for every map and reduce task. The three states that a task can have are idle, in progress or completed. It also has the location of the buffered files. The reduce tasks will read these buffered files \cite{rf2}.

\subsection{Fault Tolerance}
Since the MapReduce library is designed to handle large amounts of data and work over a distributed platform, it must tolerate machine failure smoothly \cite{rf2}. 

The master pings every worker after a specific interval. If the worker does not respond to the master then the worker is marked as failed by the master. If a mapper completes its task, then it is reset back to idle state and is available for rescheduling. The same happens when a mapper or reducer has failed. They are reset to idle and available for rescheduling. In case of a failed machine, the completed tasks are re-executed as the output is saved in the local disk, which would have otherwise been incomplete or inaccessible. The output for completed tasks is passed to the global file system. This means that their locations are passed to the master which handles the data from then on \cite{rf2}.  

When a mapper A has failed and its map task is re-executed by mapper B, then a notification is sent to all reducers for this re-execution. Any reducer that was waiting to read data from mapper A will read it from mapper B \cite{rf2}. 

Master can write periodic checkpoints of the meta data it maintains. This helps when the master dies, re-execution can be started from the last checkpoint. When there is only one master, the chances that it dies are slim. User can check the state of master and retry the MapReduce operation when the master fails \cite{rf2}.

While handling large chunks of data, network bandwidth can the scarce resource in the distributed environment. If not handled properly it will bottleneck the entire operation. Therefore the local disks, of the cluster machines, can also hold the input data. As we have seen earlier that the entire file is divided into chunks of 16 to 64 megabytes. Several copies of each block is stored in different machines. This information is also provided to the master node, which oversees the entire operation. When the MapReduce operation is in process, the input data is locally read and the network bandwidth is not consumed. A worker performs different tasks and this helps in load balancing and also speeds up the recovery in case of a failure \cite{rf2}.  
\section{Performance}
The total execution time for the MapReduce operation can be increased mainly due to straggler. Straggler is a machine that takes large time to complete a simple map or reduce task. This generally happens during the end of the MapReduce operation, when the reading of input data is slowest. To handle this situation, the master node schedules reserve executions for the left over tasks. The task is deemed completed when it is completed by both the  primary and backup machine  \cite{rf2}. 

 The arrangement of clusters and the specification of machines used also affect the performance. In the idle state the memory for the machines should be freed for better results. In case of a 1 terabyte file the data is split into 64 megabyte blocks. Therefore there will be 15000 map tasks and reducer can be user decided \cite{rf2}. 

The performance can also be affected by the type of operation we choose. In case of a normal execution where backup tasks are used in case of stragglers, the performance is good. The run time is increased when the backup tasks are not executed. This is when stragglers push the run-time further. In the third scenario when few tasks are killed towards the end the performance is good and similar to the normal execution. In some cases it can be better than the normal execution \cite{rf2}.  
\section{Conclusion}
The use MapReduce programming model has propelled a new phase better usage of Big Data. The processing of such large amount of data usually required high end machines with large memories and top of the line processors. This can now be done using common machine but in a distributed environment. This model has also improved the fault tolerance as well as the run time for processing such large data. Such a model has also helped in usage of large machine learning algorithms, clustering problems, large scale graph computation and extraction of large web content. Google uses this model for large scale indexing by creating inverted index. The computation from MapReduce model is efficient so we need not look into the complex internal workings and can create better abstracted programs. It also provides parallelization, load optimization and load balancing. Such a model can also be re scaled to thousands of machine, thus making it future ready. 

%\section{References}

\begin{acks}

  The authors would like to thank Dr. Gregor von Laszewski for his support and suggestions to write this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\DONE{just including one so it compiles }


\DONE{ALL REFRENCES NEED TO BE USING

[1] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung.
The Google File System. In 19th Symposium on Operating
Systems Principles, pages 29-43, Lake George,
New York, 2003.

[2] Jeffery Dean and Sanjay Ghemawat.
MapReduce: Simplified Data Processing on Large Clusters. Google Inc, OSDI, 2004

[3] Seema Maitreya and C.K. Jhab.
MapReduce: Simplified Data Analysis of Big
Data. Procedia Computer Science 57, Elseiver, 2015

[4] Maitrey S, Jha. An Integrated Approach for CURE Clustering using Map-Reduce Technique. In Proceedings of Elsevier, ISBN 978-81-
910691-6-3,2nd August 2013

}


\appendix

We include an appendix with common issues that we see when students
submit papers. One particular important issue is not to use the
underscore in bibtex labels. Sharelatex allows this, but the
proceedings script we have does not allow this.

When you submit the paper you need to address each of the items in the
issues.tex file and verify that you have done them. Please do this
only at the end once you have finished writing the paper. To d this
cange TODO with DONE. However if you check something on with DONE, but
we find you actually have not executed it correcty, you will receive
point deductions. Thus it is important to do this correctly and not
just 5 minutes before the deadline. It is better to do a late
submission than doing the check in haste.


\end{document}
